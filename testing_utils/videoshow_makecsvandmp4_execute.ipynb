{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import cv2\n",
    "import keras\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image \n",
    "import pickle\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from scipy.misc import imread, imresize\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ssd_utils import BBoxUtility\n",
    "from ssd import SSD300 as SSD\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "video_path = 'stopsigns.mp4'\n",
    "weights_path = '../checkpoints/weights.43-6.69.hdf5'\n",
    "class_names = [\"background\", \"stop\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTest(object):\n",
    "    def __init__(self, class_names, model, input_shape):\n",
    "        self.class_names = class_names\n",
    "        self.num_classes = len(class_names)\n",
    "        self.model = model\n",
    "        self.input_shape = input_shape\n",
    "        self.bbox_util = BBoxUtility(self.num_classes)\n",
    "        \n",
    "        # Create unique and somewhat visually distinguishable bright\n",
    "        # colors for the different classes.\n",
    "        self.class_colors = []\n",
    "        for i in range(0, self.num_classes):\n",
    "            # This can probably be written in a more elegant manner\n",
    "            hue = 255*i/self.num_classes\n",
    "            col = np.zeros((1,1,3)).astype(\"uint8\")\n",
    "            col[0][0][0] = hue\n",
    "            col[0][0][1] = 128 # Saturation\n",
    "            col[0][0][2] = 255 # Value\n",
    "            cvcol = cv2.cvtColor(col, cv2.COLOR_HSV2BGR)\n",
    "            col = (int(cvcol[0][0][0]), int(cvcol[0][0][1]), int(cvcol[0][0][2]))\n",
    "            self.class_colors.append(col) \n",
    "        \n",
    "    def run(self, video_path = 0, start_frame = 0, conf_thresh = 0.6):\n",
    "        vid = cv2.VideoCapture(video_path)\n",
    "        if not vid.isOpened():\n",
    "            raise IOError((\"Couldn't open video file or webcam. If you're \"\n",
    "            \"trying to open a webcam, make sure you video_path is an integer!\"))\n",
    "        \n",
    "        # Compute aspect ratio of video     \n",
    "        vidw = vid.get(3)\n",
    "        vidh = vid.get(4)\n",
    "        vidar = vidw/vidh\n",
    "        \n",
    "        # Skip frames until reaching start_frame\n",
    "        if start_frame > 0:\n",
    "            vid.set(cv2.cv.CV_CAP_PROP_POS_MSEC, start_frame)\n",
    "            \n",
    "        accum_time = 0\n",
    "        curr_fps = 0\n",
    "        fps = \"FPS: ??\"\n",
    "        prev_time = timer()\n",
    "            \n",
    "        while True:\n",
    "            retval, orig_image = vid.read()\n",
    "            if not retval:\n",
    "                print(\"Done!\")\n",
    "                return\n",
    "                \n",
    "            im_size = (self.input_shape[0], self.input_shape[1])    \n",
    "            resized = cv2.resize(orig_image, im_size)\n",
    "            rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Reshape to original aspect ratio for later visualization\n",
    "            # The resized version is used, to visualize what kind of resolution\n",
    "            # the network has to work with.\n",
    "            to_draw = cv2.resize(resized, (int(self.input_shape[0]*vidar), self.input_shape[1]))\n",
    "            \n",
    "            # Use model to predict \n",
    "            inputs = [image.img_to_array(rgb)]\n",
    "            tmp_inp = np.array(inputs)\n",
    "            x = preprocess_input(tmp_inp)\n",
    "            \n",
    "            y = self.model.predict(x)\n",
    "            \n",
    "            \n",
    "            # This line creates a new TensorFlow device every time. Is there a \n",
    "            # way to avoid that?\n",
    "            results = self.bbox_util.detection_out(y)\n",
    "            \n",
    "            if len(results) > 0 and len(results[0]) > 0:\n",
    "                # Interpret output, only one frame is used \n",
    "                det_label = results[0][:, 0]\n",
    "                det_conf = results[0][:, 1]\n",
    "                det_xmin = results[0][:, 2]\n",
    "                det_ymin = results[0][:, 3]\n",
    "                det_xmax = results[0][:, 4]\n",
    "                det_ymax = results[0][:, 5]\n",
    "\n",
    "                top_indices = [i for i, conf in enumerate(det_conf) if conf >= conf_thresh]\n",
    "\n",
    "                top_conf = det_conf[top_indices]\n",
    "                top_label_indices = det_label[top_indices].tolist()\n",
    "                top_xmin = det_xmin[top_indices]\n",
    "                top_ymin = det_ymin[top_indices]\n",
    "                top_xmax = det_xmax[top_indices]\n",
    "                top_ymax = det_ymax[top_indices]\n",
    "\n",
    "                for i in range(top_conf.shape[0]):\n",
    "                    xmin = int(round(top_xmin[i] * to_draw.shape[1]))\n",
    "                    ymin = int(round(top_ymin[i] * to_draw.shape[0]))\n",
    "                    xmax = int(round(top_xmax[i] * to_draw.shape[1]))\n",
    "                    ymax = int(round(top_ymax[i] * to_draw.shape[0]))\n",
    "\n",
    "                    # Draw the box on top of the to_draw image\n",
    "                    class_num = int(top_label_indices[i])\n",
    "                    cv2.rectangle(to_draw, (xmin, ymin), (xmax, ymax), \n",
    "                                  self.class_colors[class_num], 2)\n",
    "                    text = self.class_names[class_num] + \" \" + ('%.2f' % top_conf[i])\n",
    "\n",
    "                    text_top = (xmin, ymin-10)\n",
    "                    text_bot = (xmin + 80, ymin + 5)\n",
    "                    text_pos = (xmin + 5, ymin)\n",
    "                    cv2.rectangle(to_draw, text_top, text_bot, self.class_colors[class_num], -1)\n",
    "                    cv2.putText(to_draw, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0,0,0), 1)\n",
    "            \n",
    "            # Calculate FPS\n",
    "            # This computes FPS for everything, not just the model's execution \n",
    "            # which may or may not be what you want\n",
    "            curr_time = timer()\n",
    "            exec_time = curr_time - prev_time\n",
    "            prev_time = curr_time\n",
    "            accum_time = accum_time + exec_time\n",
    "            curr_fps = curr_fps + 1\n",
    "            if accum_time > 1:\n",
    "                accum_time = accum_time - 1\n",
    "                fps = \"FPS: \" + str(curr_fps)\n",
    "                curr_fps = 0\n",
    "            \n",
    "            # Draw FPS in top left corner\n",
    "            cv2.rectangle(to_draw, (0,0), (50, 17), (255,255,255), -1)\n",
    "            cv2.putText(to_draw, fps, (3,10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0,0,0), 1)\n",
    "            videostream = cv2.cvtColor(to_draw, cv2.COLOR_BGR2RGB)\n",
    "            axis('off')\n",
    "            title('output stream')\n",
    "            imshow(videostream)\n",
    "            show()\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a38661fd282e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvid_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvid_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e420134338cd>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, video_path, start_frame, conf_thresh)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# This line creates a new TensorFlow device every time. Is there a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# way to avoid that?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ssd/ssd_utils.py\u001b[0m in \u001b[0;36mdetection_out\u001b[0;34m(self, predictions, background_label_id, keep_top_k, confidence_threshold)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             decode_bbox = self.decode_boxes(mbox_loc[i],\n\u001b[0;32m--> 212\u001b[0;31m                                             mbox_priorbox[i], variances[i])\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbackground_label_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ssd/ssd_utils.py\u001b[0m in \u001b[0;36mdecode_boxes\u001b[0;34m(self, mbox_loc, mbox_priorbox, variances)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mdecode_bbox_center_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmbox_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior_width\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mdecode_bbox_center_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprior_center_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdecode_bbox_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmbox_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mdecode_bbox_width\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mprior_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdecode_bbox_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmbox_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_shape = (300,300,3)\n",
    "NUM_CLASSES = len(class_names)\n",
    "model = SSD(input_shape, num_classes=NUM_CLASSES)\n",
    "model.load_weights(weights_path) \n",
    "vid_test = VideoTest(class_names, model, input_shape)\n",
    "vid_test.run(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
